{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QVySOmftXebs"
      },
      "source": [
        "# Import the required modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5mjuEWHW7B4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YtnKuct7YKP0"
      },
      "source": [
        "# Number_of_samples determine how many samples from the attack and normal dataset should be read and used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAaL5nHTYQbg"
      },
      "outputs": [],
      "source": [
        "number_of_samples = 50000"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v8jVPRfyYUwa"
      },
      "source": [
        "# Read data from attack and normal datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ7YXhoNYibW"
      },
      "outputs": [],
      "source": [
        "data_attack = pd.read_csv('/content/dataset_attack.csv', nrows=number_of_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2r1Kc5-YvUm"
      },
      "outputs": [],
      "source": [
        "data_normal = pd.read_csv('/content/dataset_normal.csv', nrows=number_of_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTrY3OCaYyPA"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_normal.columns = ['frame.len', 'frame.protocols', 'ip.hdr_len',\n",
        "                       'ip.len', 'ip.flags.rb', 'ip.flags.df', 'p.flags.mf', 'ip.frag_offset',\n",
        "                       'ip.ttl', 'ip.proto', 'ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport',\n",
        "                       'tcp.len', 'tcp.ack', 'tcp.flags.res', 'tcp.flags.ns', 'tcp.flags.cwr',\n",
        "                       'tcp.flags.ecn', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push',\n",
        "                       'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.window_size',\n",
        "                       'tcp.time_delta', 'class']\n",
        "data_attack.columns = ['frame.len', 'frame.protocols', 'ip.hdr_len',\n",
        "                       'ip.len', 'ip.flags.rb', 'ip.flags.df', 'p.flags.mf', 'ip.frag_offset',\n",
        "                       'ip.ttl', 'ip.proto', 'ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport',\n",
        "                       'tcp.len', 'tcp.ack', 'tcp.flags.res', 'tcp.flags.ns', 'tcp.flags.cwr',\n",
        "                       'tcp.flags.ecn', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push',\n",
        "                       'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.window_size',\n",
        "                       'tcp.time_delta', 'class']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sj0eI8a6Y6js"
      },
      "source": [
        "# Drop unwanted columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjCi2V2xY66D"
      },
      "outputs": [],
      "source": [
        "data_normal = data_normal.drop(['ip.src', 'ip.dst', 'frame.protocols'], axis=1)\n",
        "data_attack = data_attack.drop(['ip.src', 'ip.dst', 'frame.protocols'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeKQI1XdY_kb"
      },
      "outputs": [],
      "source": [
        "\n",
        "features = ['frame.len', 'ip.hdr_len', 'ip.len', 'ip.flags.rb', 'ip.flags.df', 'p.flags.mf', 'ip.frag_offset',\n",
        "            'ip.ttl', 'ip.proto', 'tcp.srcport', 'tcp.dstport', 'tcp.len', 'tcp.ack', 'tcp.flags.res',\n",
        "            'tcp.flags.ns', 'tcp.flags.cwr', 'tcp.flags.ecn', 'tcp.flags.urg', 'tcp.flags.ack',\n",
        "            'tcp.flags.push', 'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.window_size',\n",
        "            'tcp.time_delta']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHqxMGhkZGfQ"
      },
      "outputs": [],
      "source": [
        "X_normal = data_normal[features].values\n",
        "X_attack = data_attack[features].values\n",
        "Y_normal = data_normal['class']\n",
        "Y_attack = data_attack['class']\n",
        "X = np.concatenate((X_normal, X_attack))\n",
        "Y = np.concatenate((Y_normal, Y_attack))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jvZpzKTDZOqj"
      },
      "source": [
        "# Standardise the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6axQXcTwZTvV"
      },
      "outputs": [],
      "source": [
        "scalar = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
        "scalar.fit(X)\n",
        "X = scalar.transform(X)\n",
        "X[np.isnan(X)] = 0\n",
        "X[np.isinf(X)] = 0\n",
        "\n",
        "for i in range(0, len(Y)):\n",
        "    if Y[i] == \"attack\":\n",
        "        Y[i] = 0\n",
        "    else:\n",
        "        Y[i] = 1\n",
        "\n",
        "le = LabelEncoder()\n",
        "Y = le.fit_transform(Y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ2XQsKLIov-"
      },
      "source": [
        "# Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY5-eFIhaSth"
      },
      "outputs": [],
      "source": [
        "features = len(X[0])\n",
        "samples = X.shape[0]\n",
        "train_len = 25\n",
        "input_len = samples - train_len\n",
        "I = np.zeros((samples - train_len, train_len, features))\n",
        "\n",
        "for i in range(input_len):\n",
        "    temp = np.zeros((train_len, features))\n",
        "    for j in range(i, i + train_len - 1):\n",
        "        temp[j - i] = X[j]\n",
        "    I[i] = temp"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Splitting the dataset into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    \n",
        "X_train, X_test, Y_train, Y_test = train_test_split(I, Y[train_len:100000], test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnbAjpVXKW-v",
        "outputId": "29f02e80-2ad0-4954-a1b7-23623fdec269"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rGkMDZ65I7BO"
      },
      "source": [
        "# Three kinds of D4C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE9Bhd3TqkSM"
      },
      "outputs": [],
      "source": [
        "class BiD4C(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(BiD4C, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.rnn = nn.LSTM(input_size=64, hidden_size=hidden_size, bidirectional=True, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, out):\n",
        "        out = out.permute(0, 2, 1)  # 将数据从 (batch_size, seq_len, input_size) 转换为 (batch_size, input_size, seq_len)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = out.permute(0, 2, 1)  # 将数据从 (batch_size, input_size, seq_len) 转换为 (batch_size, seq_len, hidden_size)\n",
        "        out, _ = self.rnn(out)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc1(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz14HwrnJEZP"
      },
      "source": [
        "## Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uds3A_RHb1YF"
      },
      "outputs": [],
      "source": [
        "input_size = features\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 40\n",
        "batch_size = 128"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xGmpdelxJQQw"
      },
      "source": [
        "# Set model, optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23qFaY66b3Z-"
      },
      "outputs": [],
      "source": [
        "model = BiD4C(input_size, hidden_size, output_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Data & Set Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxzm1o8nb75z"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BtmeBFvb_BG"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(Y_train).float())\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie55jCvtK23u"
      },
      "outputs": [],
      "source": [
        "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(Y_test).float())\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PwA_AEigyZ3"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "test_accuracies = []"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "okktrpNbN2PH"
      },
      "source": [
        "# Train the model on the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTZ0R0V9o1lP",
        "outputId": "2e704728-e934-44cc-c4f8-84064393fb9b"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0.0\n",
        "  # use a variable to store the number of batches per validation\n",
        "  val_freq = 125\n",
        "  for i, data in enumerate(train_loader, 1): # start the index from 1\n",
        "    inputs, labels = data\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs.squeeze(), labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    # check if the current batch is a multiple of val_freq\n",
        "    if i % val_freq == 0:\n",
        "      # perform validation\n",
        "      with torch.no_grad():\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for data in test_loader:\n",
        "          inputs, labels = data\n",
        "          outputs = model(inputs)\n",
        "          predicted = (outputs.squeeze() > 0.5).float()\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        \n",
        "        test_accuracy = 100 * correct / total\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print('Epoch [{}/{}], Batch [{}/{}], Train Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "        .format(epoch + 1, num_epochs, i, len(train_loader), running_loss / val_freq, test_accuracy))\n",
        "      \n",
        "      # reset the running loss\n",
        "      running_loss = 0.0\n",
        "\n",
        "      # switch back to train mode\n",
        "      model.train()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik4Xo8VyONDA"
      },
      "source": [
        "# Validated on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TAAjquicWdl",
        "outputId": "70a1e59b-9399-493a-d8be-8df0cf6f94ff"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs.squeeze() > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test data: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Re11ltSaOQyY"
      },
      "source": [
        "# Draw the change graph of loss and correct rate during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "5rYYlHVScdVe",
        "outputId": "c3f259ae-76bf-4478-ae17-fb21bec4e7bb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Create a figure object with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(2)\n",
        "# Plot training loss values on the first subplot\n",
        "ax1.plot(train_losses)\n",
        "# Set title, labels and legend\n",
        "ax1.set_title('D4C Recurrent Network Loss')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_xlabel('1/5 Epoch')\n",
        "ax1.legend(['Train Loss'], loc='upper right')\n",
        "# Plot test accuracy values on the second subplot\n",
        "ax2.plot(test_accuracies)\n",
        "# Set title, labels and legend\n",
        "ax2.set_title('D4C Recurrent Network Accuracy')\n",
        "ax2.set_ylabel('Accuracy(%)')\n",
        "ax2.set_xlabel('1/5 Epoch')\n",
        "ax2.legend(['Test Accuracy'], loc='lower right')\n",
        "# Adjust the layout of the figure\n",
        "fig.tight_layout()\n",
        "# Save the figure as an image file\n",
        "plt.savefig('D4C Loss and Accuracy.png')\n",
        "# Show the figure\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4i7pwG72Oo4h"
      },
      "source": [
        "# Compute and visualize the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrDltoPwckT_"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predict = model(torch.from_numpy(X_test).float()).numpy()\n",
        "\n",
        "predictn = predict.flatten().round()\n",
        "predictn = predictn.tolist()\n",
        "Y_testn = Y_test.tolist()\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(Y_testn, predictn).ravel()\n",
        "to_heat_map = [[tn, fp], [fn, tp]]\n",
        "to_heat_map = pd.DataFrame(to_heat_map, index=[\"Attack\", \"Normal\"], columns=[\"Attack\", \"Normal\"])\n",
        "ax = sns.heatmap(to_heat_map, annot=True, fmt=\"d\")\n",
        "\n",
        "figure = ax.get_figure()\n",
        "figure.savefig('D4C Confusion Matrix.png', dpi=400)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ryztrIMeOwbD"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoGOaAZwcuFj"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'D4C Model.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
